import os
import sqlite3
from datetime import datetime
import pandas as pd
from typing import List, Dict, Any, TypedDict, Annotated
import operator
import json
import uuid
from dotenv import load_dotenv

from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.tools import tool, InjectedToolCallId
from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, SystemMessage, ToolMessage
from langgraph.graph import StateGraph, END
from langgraph.prebuilt import ToolNode

from expert_system import ContextGuard  # ExpertEngine removed - now using pure LLM analysis

# Load env vars
load_dotenv()

# Path to DB
DB_FILE = os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), 'ads_data.sqlite')

# Configuration
MAIN_MODEL_NAME = os.getenv("MAIN_MODEL_NAME")
SUB_MODEL_NAME = os.getenv("SUB_MAIN_MODEL_NAM")
BASE_URL = os.getenv("BASE_URL")
API_KEY = os.getenv("API_KEY")
MAX_CONTEXT_CHARACTERS = int(os.getenv("MAX_CONTEXT_CHARACTERS", 30000))

# Initialize LLMs Globally
main_llm = ChatOpenAI(
    model=MAIN_MODEL_NAME,
    base_url=BASE_URL,
    api_key=API_KEY,
    temperature=0,
    streaming=True,
    request_timeout=60 # Force fallback if thinking takes too long
)

# --- Table Expert Knowledge ---
# This dict provides specific metric descriptions and diagnostic focus for the sub-agent
TABLE_EXPERT_KNOWLEDGE = {
    "Anomalies": {
        "title": "Anomaly Guard (å¼‚å¸¸å·¡æ£€ä¸“å®¶)",
        "focus": "å…¨è´¦æˆ·è‡ªåŠ¨å·¡æ£€ï¼Œè¯†åˆ«æ•ˆç‡å¼‚å¸¸ä¸å¢é•¿åœæ»ã€‚",
        "metrics_desc": "campaign: ç³»åˆ—å, roas: å¹¿å‘Šæ”¯å‡ºå›æŠ¥ç‡, cpa: å•æ¬¡è½¬åŒ–æˆæœ¬, conversions: è½¬åŒ–æ•°",
        "expert_rules": """
        - **ROAS å´©ç›˜**: 3å¤© ROAS < è´¦æˆ·å‡å€¼ 80% -> è§¦å‘æ·±åº¦è¯Šæ–­
        - **CPA é£™å‡**: 3å¤© CPA > è´¦æˆ·å‡å€¼ 125% -> è§¦å‘æˆæœ¬å¼‚å¸¸è­¦æŠ¥
        - **å¢é•¿åœæ»**: 7å¤©åŒæ¯”è½¬åŒ–é›¶å¢é•¿ -> æ ‡è®°å¢é•¿é£é™©
        - **é£æ§ä¿æŠ¤**: å¤§ä¿ƒæœŸ/å†·å¯åŠ¨æœŸè‡ªåŠ¨é™çº§é£é™©åŠ¨ä½œ
        """
    },
    "Campaigns": {
        "title": "Campaign Manager (å¹¿å‘Šç³»åˆ—ç®¡ç†ä¸“å®¶)",
        "focus": "å¹¿å‘Šç³»åˆ—æ•´ä½“å¥åº·åº¦è¯„ä¼°ä¸ä¼˜åŒ–å»ºè®®ã€‚",
        "metrics_desc": "campaign: åç§°, cost: æ¶ˆè€—, conversions: è½¬åŒ–, roas: ROAS, cpa: CPA",
        "expert_rules": """
        - **é¢„ç®—æ•ˆç‡**: é¢„ç®—æ¶ˆè€—ç‡ < 70% -> æ£€æŸ¥å®šå‘æˆ–å‡ºä»·
        - **è½¬åŒ–è´¨é‡**: è½¬åŒ–ä»·å€¼/è½¬åŒ–æ•° æŒç»­ä¸‹é™ -> æ£€æŸ¥è½åœ°é¡µæˆ–å—ä¼—
        - **ç³»åˆ—ç»“æ„**: åŒç³»åˆ—å¹¿å‘Šç»„æ•°é‡ > 10 -> å»ºè®®ç²¾ç®€ç»“æ„
        """
    },
    "Products": {
        "title": "Product Specialist (äº§å“ä¸“å®¶)",
        "focus": "å•†å“/SKU å±‚çº§æ•ˆç‡åˆ†æã€‚",
        "metrics_desc": "product_title: å•†å“å, item_id: SKU, cost: æ¶ˆè€—, conversions: è½¬åŒ–, roas: ROAS",
        "expert_rules": """
        - **åƒµå°¸å•†å“**: æ¶ˆè€— > $80 ä¸” 0 è½¬åŒ– -> å»ºè®®æ’é™¤
        - **å†·å¯åŠ¨ä¿æŠ¤**: æ–°å“æ¶ˆè€— < $30 -> æš‚ä¸ä¼˜åŒ–
        - **é¢„ç®—éœ¸å **: å•å“å é¢„ç®— > 85% -> è­¦å‘Šæµ‹è¯•é¥¥é¥¿é£é™©
        """
    },
    "asset": {
        "title": "Creative Asset Expert (ç´ æåˆ›æ„ä¸“å®¶)",
        "focus": "åˆ›æ„ç´ ææ•ˆæœè¯„ä¼°ä¸è½®æ¢å»ºè®®ã€‚",
        "metrics_desc": "asset_name: ç´ æå, asset_type: ç±»å‹, cost: æ¶ˆè€—, conversions: è½¬åŒ–, ctr: ç‚¹å‡»ç‡",
        "expert_rules": """
        - **ç–²åŠ³æ£€æµ‹**: CTR è¿ç»­ 7 å¤©ä¸‹é™ > 20% -> å»ºè®®æ›´æ¢ç´ æ
        - **æ•ˆæœåˆ†å±‚**: æŒ‰ ROAS åˆ†ä¸º Top/Middle/Bottom ä¸‰æ¡£
        - **æ ¼å¼å»ºè®®**: è§†é¢‘ç´ æ CTR é€šå¸¸é«˜äºé™æ€å›¾ï¼Œæ³¨æ„å¯¹æ¯”
        """
    },
    "location": {
        "title": "Location & Geo Expert (åœ°åŸŸä¸“å®¶)",
        "focus": "åœ°ç†ä½ç½®æŠ•æ”¾æ•ˆç‡åˆ†æã€‚",
        "metrics_desc": "location: ä½ç½®, cost: æ¶ˆè€—, conversions: è½¬åŒ–, roas: ROAS",
        "expert_rules": """
        - **é»‘æ´æ£€æµ‹**: æ¶ˆè€— >= $100 ä¸” 0 è½¬åŒ– -> å»ºè®®æ’é™¤
        - **æ•ˆç‡é£é™©**: CPA >= 2x å‡å€¼ -> å»ºè®®é™ä½å‡ºä»· 30%
        - **è§‚å¯ŸæœŸ**: æ¶ˆè€— < $50 æˆ– ç‚¹å‡» < 50 -> æ•°æ®ä¸è¶³ï¼Œç»§ç»­è§‚å¯Ÿ
        """
    },
    "age": {
        "title": "Age Demographics Expert (å¹´é¾„åˆ†å±‚ä¸“å®¶)",
        "focus": "Audit demographic efficiency with high statistical stability.",
        "metrics_desc": "age: Range, bid_adj: Existing Modifier, cost: Spend, conversions: Units, ctr: CTR",
        "expert_rules": """
        - **Tier 1: Observation (Low Sample)**: If 'clicks' < 25, status is "Too early to optimize". Only note extreme CTR anomalies.
        - **Tier 2: Risk (Delayed Conv Guard)**: If segment is < 7 days old, do not recommend exclusion. Only minor bid reduction if CPA > 2x avg.
        - **Tier 3: Actionable (High Confidence)**: Spend > 2x Account CPA AND 0 Conv over 14+ days -> Recommend -50% bid or exclusion.
        - **Unknown Guard**: Protected status. Do not exclude unless spend is 3x higher than converted segments with 0 ROAS.
        """
    },
    "gender": {
        "title": "Gender Demographics Expert (æ€§åˆ«åˆ†å±‚ä¸“å®¶)",
        "focus": "Identify structural gender imbalances.",
        "metrics_desc": "gender: Category, cost: Spend, conversions: Units, ctr: CTR",
        "expert_rules": """
        - **Confidence**: Requires min 100 clicks or 10 conversions for major advice.
        - **Protection**: If one gender has high CTR but 0 Conv, check if Landing Page is gender-neutral before excluding.
        """
    },
    "search_term": {
        "title": "Search Term Analyst (æœç´¢è¯ä¸“å®¶)",
        "focus": "Aggressive junk filtering with Brand Protection.",
        "metrics_desc": "search_term: query, match_type: Match, cost: Spend, conversions: Conv, roas: ROAS",
        "expert_rules": """
        - **Brand Umbrella**: If search_term contains Brand/Product core name, mark as "Strategic Asset". Keep even if 0 ROAS for now.
        - **Junk Patterns**: Immediate 'Critical' for terms like 'free', 'repair', 'whatsapp', 'support', 'login' (Non-sales intent).
        - **Broad Match Audit**: If > 60% of waste flows through 'Broad' match, recommend shifting to Phrase/Exact.
        - **Tiering**: Spend > 1.5x CPA + 0 Conv -> 'High Confidence' Negative Recommendation.
        """
    },
    "location_by_cities_all_campaign": {
        "title": "Geography Analyst (åœ°åŸŸ/åŸå¸‚ä¸“å®¶)",
        "focus": "Three-tier regional auditing.",
        "metrics_desc": "matched_location: Location, cost: Spend, conversions: Conv, roas: ROAS",
        "expert_rules": """
        - **Tier 1: High Confidence Blackhole**: Spend >= $100 AND 0 Conv AND Historical 30d Conv = 0 -> Recommend Exclusion.
        - **Tier 2: Efficiency Risk**: CPA >= 2x Avg CPA -> Recommend -30% bid reduction.
        - **Tier 3: Observation**: Spend < $50 OR Clicks < 50 -> Status "Observing". Data too sparse for regional exclusion.
        """
    },
    "ad_schedule": {
        "title": "Time & Schedule Analyst (åˆ†æ—¶ä¸“å®¶)",
        "focus": "Peak/Trough pattern identification with stability guard.",
        "metrics_desc": "day_and_time: Slot, cost: Spend, conversions: Conv, roas: ROAS",
        "expert_rules": """
        - **Stability Rule**: Minimum 100 clicks per slot (over 30 days) required for -50% modifier recommendation.
        - **Delayed Return Protection**: Be cautious with 00:00-05:00 slots as conversions often attribute late. 
        - **Action**: Only target extreme 'Midnight Waste' (Spend > 3x CPA, 0 Conv) for aggressive exclusion.
        """
    },
    "audience": {
        "title": "Audience Segment Analyst (å—ä¼—ä¸“å®¶)",
        "focus": "Signal-to-noise auditing.",
        "metrics_desc": "audience_segment: Signal, cost: Spend, conversions: Conv, roas: ROAS",
        "expert_rules": """
        - **Tiering**: Focus on high-spend zero-ROI 'In-market' segments.
        - **Confidence**: Require min 5 Conversions before recommending 'Targeting' instead of 'Observation'.
        """
    },
    "product": {
        "title": "Product SKU Analyst (äº§å“/è´§æ¶ä¸“å®¶)",
        "focus": "Zombie detection and Cold Start Protection.",
        "metrics_desc": "title: Name, item_id: SKU, cost: Spend, conversions: Conv, roas: ROAS",
        "expert_rules": """
        - **Cold Start Protection**: New SKUs (Total Spend < $30) are 'Protected'. Do not flag as Zombie yet.
        - **High-Confidence Zombie**: Spend > $80 AND 0 Conv -> Recommend Status 'Excluded' in Listing Group.
        - **Budget Hegemony**: If 1 product takes > 85% budget, flag as "Testing Starvation Risk".
        """
    },
    "channel": {
        "title": "PMax Channel Analyst (PMax æ¸ é“ä¸“å®¶)",
        "focus": "Cross-channel subsidy auditing.",
        "metrics_desc": "channels: Type, cost: Spend, conversions: Conv, roas: ROAS",
        "expert_rules": """
        - **Subsidy Check**: Flag if 'Shopping' (Feed) is subsidizing > 40% waste in 'Video/Display'.
        - **Structure Risk**: If Video Spend > 30% AND Video CPA > 2.5x Target -> High Risk Recommendation.
        """
    }
}

sub_llm = ChatOpenAI(
    model=SUB_MODEL_NAME,
    base_url=BASE_URL,
    api_key=API_KEY,
    temperature=0.1, # Slightly creative for reporting
    streaming=False  # Sub-agent usually returns full report
)

# --- Database Helpers ---

def get_db_connection():
    return sqlite3.connect(DB_FILE)

def query_db(query: str, params: tuple = ()) -> List[Dict[str, Any]]:
    conn = get_db_connection()
    conn.row_factory = sqlite3.Row
    try:
        cursor = conn.cursor()
        cursor.execute(query, params)
        rows = cursor.fetchall()
        return [dict(row) for row in rows]
    except Exception as e:
        print(f"DB Error: {e}")
        print(f"Failed Query: {query[:200]}...")  # Show first 200 chars
        print(f"Params: {params}")
        return []
    finally:
        conn.close()

def query_value(query: str, params: tuple = ()):
    """Helper to get a single value from DB"""
    res = query_db(query, params)
    if res:
        return list(res[0].values())[0]
    return 0

def safe_truncate_data(data_list: List[Dict], max_chars: int) -> str:
    """
    Incrementally adds rows to the JSON output until max_chars is reached.
    Ensures we don't break the model context while keeping the most important (top-ranked) data.
    """
    if not data_list:
        return "[]"
    
    truncated_list = []
    current_size = 0
    # Reserve room for metadata and surrounding JSON markers
    available_chars = max_chars - 500 
    
    for row in data_list:
        row_json = json.dumps(row, ensure_ascii=False)
        if current_size + len(row_json) + 2 > available_chars:
            break
        truncated_list.append(row)
        current_size += len(row_json) + 2
        
    result = {
        "data": truncated_list,
        "metadata": {
            "total_rows_queried": len(data_list),
            "rows_included": len(truncated_list),
            "is_truncated": len(truncated_list) < len(data_list),
            "truncation_warning": "DATA TRUNCATED DUE TO CONTEXT LIMIT" if len(truncated_list) < len(data_list) else "None"
        }
    }
    return json.dumps(result, ensure_ascii=False, indent=2)

# --- Tools Definition ---

def analyze_specific_table(campaign_name: str, table_name: str, start_date: str = None, end_date: str = None) -> str:
    """
    Calls a specialized sub-agent to analyze a specific table for a campaign within a date range.
    Input: campaign_name (exact name), table_name (e.g., 'age', 'search_term'), start_date (YYYY-MM-DD), end_date (YYYY-MM-DD).
    """
    # Strict validation: Only allow analysis if it matches expert knowledge
    if table_name not in TABLE_EXPERT_KNOWLEDGE:
        return f"Error: No expert knowledge defined for table '{table_name}'. You cannot analyze this table yet."

    # 1. Fetch Main Campaign Context (The "Big Picture")
    main_stats = query_db("SELECT cost, conversions, roas, cpa FROM campaign WHERE campaign = ?", (campaign_name,))
    context_str = "No main campaign aggregate found."
    if main_stats:
        s = main_stats[0]
        context_str = f"Main Campaign Avg: Cost ${s.get('cost')}, ROAS {s.get('roas')}, CPA ${s.get('cpa')}, Conv {s.get('conversions')}"

    campaign_col = 'campaign'
    if table_name == 'channel': 
        campaign_col = 'campaigns'
    elif table_name == 'product':
        campaign_col = '1' # Special case: product table lacks campaign pivot, disable filter

    # 2. Fetch Targeted Table Data with Date Filter
    where_conditions = []
    params = []
    
    if table_name != 'product':
        where_conditions.append(f"{campaign_col} = ?")
        params.append(campaign_name)
    
    if start_date:
        where_conditions.append("date >= ?")
        params.append(start_date)
    if end_date:
        where_conditions.append("date <= ?")
        params.append(end_date)
        
    where_clause = " AND ".join(where_conditions) if where_conditions else "1=1"
    query = f"SELECT * FROM {table_name} WHERE {where_clause} ORDER BY CAST(cost AS REAL) DESC LIMIT 15"
    table_data = query_db(query, tuple(params))
    
    if not table_data:
        return f"Found no data in '{table_name}' for campaign '{campaign_name}'."

    # 3. Load Custom Rules for this Table (if any) - ç”¨æˆ·è‡ªå®šä¹‰è§„åˆ™ä¼˜å…ˆ
    analysis_rules = ""
    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        cursor.execute("SELECT rule_prompt FROM agent_custom_rules WHERE table_name = ? AND is_active = 1", (table_name,))
        result = cursor.fetchone()
        if result and result[0]:
            # ç”¨æˆ·æœ‰è‡ªå®šä¹‰è§„åˆ™ï¼Œä½¿ç”¨è‡ªå®šä¹‰è§„åˆ™
            analysis_rules = result[0]
        conn.close()
    except Exception as e:
        print(f"Warning: Could not load custom rules for {table_name}: {e}")

    # å¦‚æœæ²¡æœ‰è‡ªå®šä¹‰è§„åˆ™ï¼Œä½¿ç”¨é»˜è®¤è§„åˆ™
    if not analysis_rules:
        expert = TABLE_EXPERT_KNOWLEDGE.get(table_name, {})
        analysis_rules = expert.get('expert_rules', 'åˆ†ææ•°æ®æ•ˆç‡å’Œå¼‚å¸¸ã€‚')

    # 4. è·å–ä¸“å®¶ä¿¡æ¯
    expert = TABLE_EXPERT_KNOWLEDGE.get(table_name, {
        "title": f"{table_name} åˆ†æä¸“å®¶",
        "focus": "æ•°æ®æ•ˆç‡åˆ†æ",
        "metrics_desc": "è¯·æ ¹æ®æ•°æ®åˆ—è‡ªè¡Œåˆ¤æ–­æŒ‡æ ‡å«ä¹‰"
    })

    # 5. çº¯ LLM åˆ†æ - æç¤ºè¯å³è§„åˆ™
    prompt = f"""
    ä½ æ˜¯ä¸“ä¸šçš„ Google Ads ä¼˜åŒ–ä¸“å®¶ï¼Œä¸“æ³¨äº: {expert['title']}ã€‚
    
    **åˆ†æç›®æ ‡**: {expert['focus']}
    **æŒ‡æ ‡è¯´æ˜**: {expert['metrics_desc']}
    
    ---
    ## âš ï¸ é‡è¦ï¼šä½ å¿…é¡»ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹è§„åˆ™è¿›è¡Œåˆ†æ
    
    {analysis_rules}
    
    ---
    **å¹¿å‘Šç³»åˆ—**: {campaign_name}
    **åˆ†æå‘¨æœŸ**: {start_date or 'ALL'} è‡³ {end_date or 'ALL'}
    **æ•´ä½“åŸºå‡†**: {context_str}
    
    **åŸå§‹æ•°æ® (æŒ‰é‡è¦æ€§æ’åº):**
    {safe_truncate_data(table_data, MAX_CONTEXT_CHARACTERS)}
    
    ---
    **è¾“å‡ºè¦æ±‚**:
    1. **ä¸¥æ ¼æ‰§è¡Œè§„åˆ™**: æŒ‰ç…§ä¸Šè¿°ã€åˆ†æè§„åˆ™ã€‘ä¸­çš„é˜ˆå€¼å’Œæ¡ä»¶è¿›è¡Œåˆ¤æ–­
    2. **çŠ¶æ€ä¸ç½®ä¿¡åº¦**: æ˜ç¡®æ ‡æ³¨ "çŠ¶æ€: [è§‚å¯ŸæœŸ | å¯è¡ŒåŠ¨]" å’Œ "ç½®ä¿¡åº¦: [é«˜|ä¸­|ä½]"
    3. **å…·ä½“è¡ŒåŠ¨å»ºè®®**: ç»™å‡ºå…·ä½“çš„ä¼˜åŒ–å»ºè®®ï¼ˆå¦‚æ’é™¤è¯ã€è°ƒæ•´å‡ºä»·ç­‰ï¼‰
    4. è¾“å‡ºæ ¼å¼ï¼šç®€æ´çš„ Markdownï¼Œä½¿ç”¨ä¸­æ–‡
    """

    res = sub_llm.invoke(prompt)
    return res.content


def scan_campaigns_for_anomalies(target_date: str = None) -> str:
    """
    Scans for anomalies using the robust 3-Day Logic (User Defined) and Expert Diagnosis.
    
    Workflow:
    1. Trigger: Detect continuity & growth issues.
    2. Guard: Check business context (Promotion, Cold Start).
    3. Experts: Run specialized diagnostic engines (SearchTerm, Channel, Product, Geo).
    4. Report: Aggregate findings into a decision-grade report.
    """
    # 1. Main Agent Trigger
    anomalies = get_campaign_anomalies_logic(target_date) 
    
    if not anomalies:
         return "âœ… æ•ˆç‡å·¡æ£€é€šè¿‡ï¼šæ²¡æœ‰å¹¿å‘Šç³»åˆ—è§¦å‘ 3 å¤© ROAS/CPA é¢„è­¦ã€‚"

    # åŠ è½½å¼‚å¸¸æ£€æµ‹è§„åˆ™ (ç”¨æˆ·è‡ªå®šä¹‰æˆ–é»˜è®¤)
    analysis_rules = ""
    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        cursor.execute("SELECT rule_prompt FROM agent_custom_rules WHERE table_name = 'Anomalies' AND is_active = 1")
        result = cursor.fetchone()
        if result and result[0]:
            analysis_rules = result[0]
        conn.close()
    except:
        pass
    
    if not analysis_rules:
        expert = TABLE_EXPERT_KNOWLEDGE.get("Anomalies", {})
        analysis_rules = expert.get("expert_rules", "")

    # å‡†å¤‡æ•°æ®ç»™ LLM
    report = []
    for a in anomalies:
        campaign_name = a.get('campaign', 'Unknown')
        campaign_type = a.get('campaign_type', 'Unknown')
        
        # æ”¶é›†ç›¸å…³æ•°æ®ä¾› LLM åˆ†æ
        related_data = {}
        
        # æœç´¢è¯æ•°æ®
        st_data = query_db("SELECT * FROM search_term WHERE campaign = ? ORDER BY CAST(cost AS REAL) DESC LIMIT 10", (campaign_name,))
        if st_data:
            related_data['search_term'] = st_data
        
        # æ¸ é“æ•°æ® (PMax)
        ch_data = query_db("SELECT * FROM channel WHERE campaigns LIKE ? ORDER BY CAST(cost AS REAL) DESC LIMIT 10", (f"%{campaign_name}%",))
        if ch_data:
            related_data['channel'] = ch_data
        
        # å•†å“æ•°æ®
        pr_data = query_db("SELECT * FROM product ORDER BY CAST(cost AS REAL) DESC LIMIT 10")
        if pr_data:
            related_data['product'] = pr_data
        
        # åœ°åŸŸæ•°æ®
        geo_data = query_db("SELECT * FROM location_by_cities_all_campaign WHERE campaign = ? ORDER BY CAST(cost AS REAL) DESC LIMIT 10", (campaign_name,))
        if geo_data:
            related_data['geo'] = geo_data

        # çº¯ LLM åˆ†æ
        prompt = f"""
        ä½ æ˜¯ Google Ads å…¨è´¦æˆ·å·¡æ£€ä¸“å®¶ã€‚è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹è§„åˆ™åˆ†æå¼‚å¸¸å¹¿å‘Šç³»åˆ—ã€‚
        
        ---
        ## âš ï¸ å¿…é¡»éµå®ˆçš„åˆ†æè§„åˆ™
        
        {analysis_rules}
        
        ---
        ## è§¦å‘å¼‚å¸¸çš„å¹¿å‘Šç³»åˆ—
        
        **ç³»åˆ—åç§°**: {campaign_name}
        **ç³»åˆ—ç±»å‹**: {campaign_type}
        **å¼‚å¸¸æ•°æ®**: {json.dumps(a, ensure_ascii=False)}
        
        ## ç›¸å…³ç»´åº¦æ•°æ®
        
        {json.dumps(related_data, ensure_ascii=False, indent=2)[:8000]}
        
        ---
        ## è¾“å‡ºè¦æ±‚
        
        1. **æ ¹æœ¬åŸå› åˆ†æ**: åˆ¤æ–­æ•ˆç‡ä¸‹é™çš„æ ¹æœ¬åŸå› ï¼ˆæµé‡è´¨é‡ã€ç»“æ„é—®é¢˜ã€å¸‚åœºå˜åŒ–ï¼‰
        2. **ä¸¥æ ¼æ‰§è¡Œè§„åˆ™**: æŒ‰ç…§ä¸Šè¿°è§„åˆ™ä¸­çš„é˜ˆå€¼è¿›è¡Œåˆ¤æ–­
        3. **å…·ä½“è¡ŒåŠ¨å»ºè®®**: ç»™å‡ºå¯ç«‹å³æ‰§è¡Œçš„ä¼˜åŒ–å»ºè®®
        4. **ç½®ä¿¡åº¦æ ‡æ³¨**: æ ‡æ³¨åˆ†æçš„ç½®ä¿¡åº¦ [é«˜|ä¸­|ä½]
        5. è¾“å‡ºæ ¼å¼ï¼šç®€æ´ Markdownï¼Œä¸­æ–‡
        """
        
        llm_analysis = sub_llm.invoke(prompt).content
        report.append(f"### âš ï¸ {campaign_name}\n\n{llm_analysis}\n")

    return "\n".join(report)


def call_pmax_agent(campaign_name: str, issues: List[str], start_date: str = None, end_date: str = None) -> str:
    """
    Calls the PMax Sub-Agent to analyze a specific Performance Max campaign within a date range.
    Performs deep dive into Channels, Products, Locations, and Search Terms.
    """
    report = [f"### ğŸ•µï¸ PMax Deep Dive: {campaign_name}"]
    report.append(f"**Trigger Issues**: {', '.join(issues)}\n")

    # A. Channel Analysis (Calculated Metrics)
    try:
        # Fetch raw metrics with date filter
        where_conditions = ["campaigns LIKE ?", "status = 'active'"]
        params = [f"%{campaign_name}%"]
        if start_date:
            where_conditions.append("date >= ?")
            params.append(start_date)
        if end_date:
            where_conditions.append("date <= ?")
            params.append(end_date)

        where_clause = " AND ".join(where_conditions)
        channels_data = query_db(
            f"""
            SELECT channels, status, cost, conversions, conv_value, clicks
            FROM channel 
            WHERE {where_clause}
            ORDER BY CAST(cost AS REAL) DESC
            """, 
            tuple(params)
        )
        
        total_pmax_cost = sum([float(c.get('cost', 0) or 0) for c in channels_data])
        
        if channels_data:
            report.append("#### ğŸ“¡ A. Channel Analysis (Data Source: channel table)")
            
            # --- PMax Traffic Washing Logic ---
            display_spend = 0
            display_roas = 0
            video_spend = 0
            
            for c in channels_data:
                channel_name = c.get('channels', 'Unknown')
                cost = float(c.get('cost', 0) or 0)
                val = float(c.get('conv_value', 0) or 0)
                
                if 'Display' in channel_name:
                    display_spend += cost
                    display_roas = (val / cost) if cost > 0 else 0
                if 'Video' in channel_name:
                    video_spend += cost
            
            display_share = (display_spend / total_pmax_cost * 100) if total_pmax_cost > 0 else 0
            
            # Logic 1: Display Waste Check
            if display_share > 35 and display_roas < 1.0:
                 report.append(f"âš ï¸ **PMax æµé‡æ´—æ ·åˆ¤å®š (Traffic Washing Detected)**")
                 report.append(f"   - **ç°è±¡**: Display Channel is consuming {display_share:.1f}% of budget with low ROAS ({display_roas:.2f}).")
                 report.append("   - **ä¸“å®¶ç»éªŒ**: è‹¥ Display æ¶ˆè€—å æ¯” > 35% ä¸”å…¶ ROAS < å…¨è´¦æˆ·å‡å€¼ 50%ï¼Œåˆ¤å®šä¸º PMax æ­£åœ¨åå™¬ä½è´¨æµé‡ (PMax is dumping budget into cheap inventory).")
                 report.append("   - **å»ºè®®**: Consider tightening audience signals or excluding placements.")
            
            # Logic 2: Cross-Channel Check
            for c in channels_data:
                channel_name = c.get('channels', 'Unknown')
                cost = float(c.get('cost', 0) or 0)
                val = float(c.get('conv_value', 0) or 0)
                roas = (val / cost) if cost > 0 else 0.0
                
                metrics_str = f"Cost: ${cost:.2f} | ROAS: {roas:.2f}"
                report.append(f"- **{channel_name}**: {metrics_str}")

            report.append("")
        else:
            report.append("â„¹ï¸ No active channel data found for this campaign.")
            report.append("")

    except Exception as e:
        report.append(f"Error in Channel Analysis: {e}")

    # B. Product Analysis (The "Shelf")
    # Logic: Structural Profitability Check
    try:
        products = query_db("SELECT title, item_id, cost, conversions, conv_value_cost FROM product ORDER BY CAST(cost AS REAL) DESC LIMIT 10")
        zombies = []
        inefficient = []
        
        for p in products:
            cost = float(p.get('cost', 0))
            conv = float(p.get('conversions', 0))
            roas = float(p.get('conv_value_cost', 0)) if p.get('conv_value_cost') else 0
            item_id = p.get('item_id', 'N/A')
            title = p.get('title', 'Unknown')
            
            # Cold Start Protection (Simulated): If cost < 30, ignore unless 0 conv for long time (not checked here)
            if cost > 30: 
                if cost > 50 and conv == 0:
                    zombies.append(f"{title} (ID: {item_id}) - Cost ${cost:.2f}, 0 Conv")
                elif cost > 20 and roas < 0.5:
                    inefficient.append(f"{title} (ID: {item_id}) - ROAS {roas:.2f}")

        report.append("#### ğŸ“¦ B. Product Analysis (Source: product table)")
        
        if zombies:
            report.append("âŒ **Zombie Products (High Cost, 0 Conv)**:")
            for z in zombies: report.append(f"  - {z}")
        if inefficient:
            report.append("âš ï¸ **ç»“æ„æ€§æŸè€—åˆ¤å®š (Structural Profit Issue)**:")
            for i in inefficient: report.append(f"  - {i}")
            report.append("   - **ä¸“å®¶ç»éªŒ**: è‹¥ä½ ROAS å•†å“é›†ä¸­åœ¨ä½æ¯›åˆ© SKUï¼Œåˆ¤å®šä¸º ç»“æ„æ€§æ¯›åˆ©é—®é¢˜è€Œéå•çº¯æµé‡é—®é¢˜ (Structural margin issue, not just traffic).")
            report.append("   - **æ³¨æ„**: Unless these are high-margin (>80%) traffic drivers, they are bleeding profit.")
        
        if not zombies and not inefficient:
             report.append("âœ… Top spending products are performing within acceptable range.")
        
        report.append("")

    except Exception as e:
        report.append(f"Error in Product Analysis: {e}")

    # C. Location Analysis
    try:
        locs = query_db("SELECT matched_location, cost, conversions FROM location_by_cities_all_campaign WHERE campaign = ? AND CAST(cost AS REAL) > 50 AND CAST(conversions AS REAL) = 0 ORDER BY CAST(cost AS REAL) DESC LIMIT 3", (campaign_name,))
        report.append("#### ğŸŒ C. Location Analysis")
        if locs:
            report.append("âŒ **Money Wasting Locations**:")
            for l in locs:
                report.append(f"- **{l.get('matched_location')}**: Cost ${l.get('cost')}, 0 Conv")
            report.append("ğŸ‘‰ **Action**: Exclude these locations in Campaign Settings.")
        else:
            report.append("âœ… No high-spend zero-conversion locations found.")
        report.append("")
    except Exception as e:
        report.append(f"Error in Location Analysis: {e}")

    # D. Search Term Analysis (PMax Search Terms)
    try:
        bad_keywords = ['free', 'repair', 'login', 'support', 'manual', 'review']
        terms = query_db("SELECT search_term, cost, conversions FROM search_term WHERE campaign = ? ORDER BY CAST(cost AS REAL) DESC LIMIT 20", (campaign_name,))
        
        found_bad_terms = []
        for t in terms:
            term = t.get('search_term', '').lower()
            cost = float(t.get('cost', 0))
            if any(bk in term for bk in bad_keywords):
                found_bad_terms.append(f"'{term}' (Cost ${cost})")
        
        report.append("#### ğŸ” D. Search Term Analysis")
        if found_bad_terms:
            report.append("âš ï¸ **Irrelevant Search Terms Detected**:")
            for ft in found_bad_terms: report.append(f"- {ft}")
            report.append("ğŸ‘‰ **Action**: Add these as Account-Level Negative Keywords.")
        else:
            report.append("âœ… No obvious junk keywords found in top spenders.")
            
    except Exception as e:
        report.append(f"Error in Search Term Analysis: {e}")

    return "\n".join(report)


def call_search_agent(campaign_name: str, issues: List[str], start_date: str = None, end_date: str = None) -> str:
    """
    Calls the Search Sub-Agent to analyze a specific Search campaign within a date range.
    Uses the flash model to deeper analyze Search Terms, Match Types, Audiences.
    """
    data_context = []
    data_context.append(f"Campaign: {campaign_name}")
    data_context.append(f"Trigger Issues: {', '.join(issues)}")

    # A. Search Terms
    try:
        where_conditions = ["campaign = ?"]
        params = [campaign_name]
        if start_date:
            where_conditions.append("date >= ?")
            params.append(start_date)
        if end_date:
            where_conditions.append("date <= ?")
            params.append(end_date)
        
        where_clause = " AND ".join(where_conditions)
        terms = query_db(f"SELECT search_term, match_type, cost, conversions, conv_value_cost FROM search_term WHERE {where_clause} ORDER BY CAST(cost AS REAL) DESC LIMIT 20", tuple(params))
        data_context.append(f"\n[Search Terms (Top 20 Impact)]: {json.dumps([dict(r) for r in terms], ensure_ascii=False)}")
    except: pass

    # B. Match Types (Broad vs Exact Logic)
    try:
        match_stats = query_db("""
            SELECT match_type, SUM(CAST(cost AS REAL)) as total_cost, SUM(CAST(conversions AS REAL)) as total_conv, SUM(CAST(conv_value AS REAL)) as total_value
            FROM search_term WHERE campaign = ? GROUP BY match_type ORDER BY total_cost DESC
        """, (campaign_name,))
        
        # Calculate CVR for Flash to use
        enhanced_stats = []
        for ms in match_stats:
            d = dict(ms)
            cost = d['total_cost'] or 0
            conv = d['total_conv'] or 0
            # Rough CVR estimation requires clicks, but we assume low CVR if High Cost Low Conv
            d['cpa'] = cost / conv if conv > 0 else 0
            enhanced_stats.append(d)
            
        data_context.append(f"\n[Match Type Stats]: {json.dumps(enhanced_stats, ensure_ascii=False)}")
    except: pass

    # C. Audiences
    try:
        audiences = query_db("SELECT audience_segment, cost, conversions, conv_value_cost FROM audience WHERE campaign = ? ORDER BY CAST(cost AS REAL) DESC LIMIT 10", (campaign_name,))
        data_context.append(f"\n[Audience Data]: {json.dumps([dict(r) for r in audiences], ensure_ascii=False)}")
    except: pass

    # Invoke Sub-Agent LLM
    prompt = f"""
    You are a specialized Search Ads Analysis Agent. Analyze the provided data for Search Campaign '{campaign_name}' and produce a concise report in Chinese.

    **Data Context:**
    {chr(10).join(data_context)}

    **Analysis Logic (Strictly apply Expert Experience):**
    1. **æœç´¢æµé‡è´¨é‡åˆ¤å®š (Search Quality)**: 
       - If 'Broad' match type has > 40% spend share AND its CPA is > 1.5x of 'Exact' match, VERDICT: "åˆ¤å®šä¸º æµé‡åŒ¹é…è´¨é‡ä¸‹æ»‘ (Match Quality Degradation)".
       - Logic: "è‹¥å¹¿æ³›åŒ¹é…å æ¯”æå‡ä¸” Search Term CVR åŒæœŸä¸‹é™ï¼Œåˆ¤å®šä¸º æµé‡åŒ¹é…è´¨é‡ä¸‹æ»‘".
    2. **Search Terms**: Identify irrelevant junk terms (negative opportunities).
    3. **Audience**: Identify high-spend (> $50) audiences with 0 conversions.

    **Output Format (Markdown):**
    ### ğŸ” Search Campaign Deep Dive: {campaign_name}
    #### 1. æ ¸å¿ƒå‘ç° (Core Findings)
    - **ä¸“å®¶åˆ¤å®š**: (Cite the Expert Verdict if applicable, e.g. "æµé‡åŒ¹é…è´¨é‡ä¸‹æ»‘")
    - **Evidence**: (Data backing the verdict)
    
    #### 2. è¯¦ç»†åˆ†æ (Analysis)
    - **åŒ¹é…ç±»å‹æ•ˆèƒ½**: (Compare Broad vs Exact/Phrase)
    - **æœç´¢è¯**: ...
    
    #### 3. ä¼˜åŒ–å»ºè®® (Actions)
    - [ ] Action 1
    """

    msg = sub_llm.invoke(prompt)
    return msg.content


# --- LangGraph Setup ---

class AgentState(TypedDict):
    messages: Annotated[List[BaseMessage], operator.add]
    selected_tables: List[str]


# --- Standalone Logic (Decoupled from AgentService) ---

def get_campaign_anomalies_logic(target_date: str = None):
    """
    Identify anomalous campaigns for a specific date (defaults to latest in DB).
    Risk Control: Returns empty if target_date falls within major promotion periods.
    """
    # --- 0. Risk Control: Promotion Protection Defaults ---
    # Format: (Start, End). Example: Black Friday / Cyber Monday.
    PROMOTION_PERIODS = [
        ('2025-11-20', '2025-12-05'), # BFCM
        ('2026-06-01', '2026-06-20'), # 618 Sale
    ]
    
    conn = get_db_connection()
    try:
        # 1. Determine the target "Today"
        if not target_date:
            cursor = conn.cursor()
            cursor.execute("SELECT MAX(date) FROM campaign")
            res = cursor.fetchone()
            target_date = res[0] if res else None
            if not target_date:
                return []
        
        # 2. Fetch raw data (Last 45 days relative to target_date)
        query = """
            SELECT date, campaign, roas, cpa, conversions, budget, campaign_type 
            FROM campaign 
            WHERE date <= ? AND date >= date(?, '-45 days')
            ORDER BY campaign, date ASC
        """
        df = pd.read_sql_query(query, conn, params=(target_date, target_date))
        
        if df.empty:
            return []

        # Clean and Convert
        df['date'] = pd.to_datetime(df['date'])
        target_dt = pd.to_datetime(target_date)
        df['roas'] = pd.to_numeric(df['roas'], errors='coerce').fillna(0)
        df['cpa'] = pd.to_numeric(df['cpa'], errors='coerce').fillna(0)
        df['conversions'] = pd.to_numeric(df['conversions'], errors='coerce').fillna(0)

        anomalies = []
        
        # Group by campaign
        for campaign_name, group in df.groupby('campaign'):
            group = group.sort_values('date')
            if len(group) < 10: 
                continue

            # The analysis target is target_dt
            last_date = target_dt
            
            # Check 3 days: T, T-1, T-2
            check_dates = [last_date - pd.Timedelta(days=i) for i in range(3)] 
            
            # Check Condition A: Efficiency for EACH of the last 3 days
            is_efficiency_bad = True
            
            for d in check_dates:
                # Specific day row
                day_row = group[group['date'] == d]
                if day_row.empty:
                    is_efficiency_bad = False; break
                
                current_roas = day_row['roas'].values[0]
                current_cpa = day_row['cpa'].values[0]

                # History: 7 days prior to 'd' -> [d-7, d-1]
                start_hist = d - pd.Timedelta(days=7)
                end_hist = d - pd.Timedelta(days=1)
                
                hist_rows = group[(group['date'] >= start_hist) & (group['date'] <= end_hist)]
                if hist_rows.empty:
                    is_efficiency_bad = False; break
                    
                avg_roas = hist_rows['roas'].mean()
                avg_cpa = hist_rows['cpa'].mean()
                
                # Criteria
                roas_bad = (avg_roas > 0) and (current_roas < avg_roas * 0.8)
                cpa_bad = (avg_cpa > 0) and (current_cpa > avg_cpa * 1.25)
                
                if not (roas_bad or cpa_bad):
                    is_efficiency_bad = False
                    break
            
            if not is_efficiency_bad:
                continue

            # Check Condition B: No Growth
            # Current Period: [T-2, T]
            # Week-over-week Previous Period: [T-9, T-7]
            current_start = last_date - pd.Timedelta(days=2)
            prev_end = last_date - pd.Timedelta(days=7)
            prev_start = prev_end - pd.Timedelta(days=2)
            
            current_conv = group[(group['date'] >= current_start) & (group['date'] <= last_date)]['conversions'].sum()
            prev_conv = group[(group['date'] >= prev_start) & (group['date'] <= prev_end)]['conversions'].sum()
            
            growth = 0
            if prev_conv > 0:
                growth = (current_conv - prev_conv) / prev_conv
            
            is_growth_bad = False
            if prev_conv > 0:
                 if growth <= 0: is_growth_bad = True
            else:
                 if current_conv == 0: is_growth_bad = True
            
            if is_growth_bad:
                # Calculate summary stats for display (3d vs prev 7d)
                curr_3d_mask = (group['date'] >= current_start) & (group['date'] <= last_date)
                prev_7d_mask = (group['date'] >= last_date - pd.Timedelta(days=9)) & (group['date'] <= last_date - pd.Timedelta(days=3))
                
                curr_roas = group[curr_3d_mask]['roas'].mean()
                prev_roas = group[prev_7d_mask]['roas'].mean()
                
                curr_cpa = group[curr_3d_mask]['cpa'].mean()
                prev_cpa = group[prev_7d_mask]['cpa'].mean()

                # Determine specific efficiency reason
                efficiency_details = []
                if prev_roas > 0 and curr_roas < prev_roas * 0.8:
                    drop_pct = (prev_roas - curr_roas) / prev_roas * 100
                    efficiency_details.append(f"ROAS -{drop_pct:.0f}%")
                if prev_cpa > 0 and curr_cpa > prev_cpa * 1.25:
                    rise_pct = (curr_cpa - prev_cpa) / prev_cpa * 100
                    efficiency_details.append(f"CPA +{rise_pct:.0f}%")
                
                reason_str = " & ".join(efficiency_details)
                if not reason_str: reason_str = "Efficiency Alert"

                # 4. Integrate Context Guard Risk Assessment
                risk_info = ContextGuard.check_risk({"campaign": campaign_name}, last_date.strftime('%Y-%m-%d'))
                
                risk_label = "ğŸ”´ Critical"
                if risk_info['status'] == "BLOCK": risk_label = "ğŸ›¡ï¸ Protected (Tag Only)"
                elif risk_info['status'] == "MARK": risk_label = "âš ï¸ Warning (Observing)"

                # 5. Get Campaign Type for Expert Routing
                camp_type = group['campaign_type'].iloc[0] if 'campaign_type' in group.columns else 'Unknown'
                
                # Determine suggested experts based on campaign type
                suggested_experts = []
                if 'search' in str(camp_type).lower():
                    suggested_experts = ['search_term', 'keyword', 'age', 'gender']
                elif 'pmax' in str(camp_type).lower() or 'performance max' in str(camp_type).lower():
                    suggested_experts = ['channel', 'product', 'location_by_cities_all_campaign']
                else:
                    suggested_experts = ['age', 'gender', 'location_by_cities_all_campaign']

                anomalies.append({
                    "id": str(campaign_name),
                    "campaign": campaign_name,
                    "campaign_type": str(camp_type),
                    "date": last_date.strftime('%Y-%m-%d'),
                    "growth_rate": growth,
                    "current_conv": float(current_conv),
                    "prev_conv": float(prev_conv),
                    # Efficiency Metrics
                    "curr_roas": float(curr_roas) if not pd.isna(curr_roas) else 0.0,
                    "prev_roas": float(prev_roas) if not pd.isna(prev_roas) else 0.0,
                    "curr_cpa": float(curr_cpa) if not pd.isna(curr_cpa) else 0.0,
                    "prev_cpa": float(prev_cpa) if not pd.isna(prev_cpa) else 0.0,
                    
                    "status": risk_label,
                    "risk_level": risk_info['status'],
                    "guard_reasons": risk_info['reasons'],
                    "suggested_experts": suggested_experts,
                    "reason": f"{reason_str} & No Growth"
                })
        
        return anomalies

    except Exception as e:
        print(f"Anomaly Detection Error: {e}")
        return []
    finally:
        conn.close()


def get_product_anomalies_logic(target_date: str = None):
    """
    Identify anomalous products for a specific date (defaults to latest in DB).
    Similar logic to campaign anomalies but adapted for product metrics.
    
    Detection criteria:
    1. High cost with low/no clicks (Zombie Products)
    2. CTR declining trend (3-day consecutive decline)
    3. Cost efficiency degradation
    """
    conn = get_db_connection()
    try:
        # 1. Determine the target "Today"
        if not target_date:
            cursor = conn.cursor()
            cursor.execute("SELECT MAX(date) FROM product")
            res = cursor.fetchone()
            target_date = res[0] if res else None
            if not target_date:
                return []
        
        # 2. Fetch raw data (Last 14 days relative to target_date)
        query = """
            SELECT date, title, item_id, cost, clicks, impr, ctr, avg_cpc
            FROM product 
            WHERE date <= ? AND date >= date(?, '-14 days')
            ORDER BY item_id, date ASC
        """
        df = pd.read_sql_query(query, conn, params=(target_date, target_date))
        
        if df.empty:
            return []

        # Clean and Convert
        df['date'] = pd.to_datetime(df['date'])
        target_dt = pd.to_datetime(target_date)
        df['cost'] = pd.to_numeric(df['cost'].astype(str).str.replace('$', '').str.replace(',', ''), errors='coerce').fillna(0)
        df['clicks'] = pd.to_numeric(df['clicks'], errors='coerce').fillna(0)
        df['impr'] = pd.to_numeric(df['impr'], errors='coerce').fillna(0)
        df['ctr'] = pd.to_numeric(df['ctr'].astype(str).str.replace('%', ''), errors='coerce').fillna(0)

        anomalies = []
        
        # Group by product (item_id)
        for item_id, group in df.groupby('item_id'):
            group = group.sort_values('date')
            if len(group) < 3:
                continue

            title = group['title'].iloc[-1] if 'title' in group.columns else 'Unknown Product'
            last_date = target_dt
            
            # Get last 3 days data
            last_3_days = group[group['date'] >= (last_date - pd.Timedelta(days=2))]
            if last_3_days.empty:
                continue
                
            # Get previous 7 days for comparison
            prev_7_days = group[(group['date'] >= (last_date - pd.Timedelta(days=9))) & 
                                (group['date'] <= (last_date - pd.Timedelta(days=3)))]
            
            # Current period metrics
            curr_cost = last_3_days['cost'].sum()
            curr_clicks = last_3_days['clicks'].sum()
            curr_ctr = last_3_days['ctr'].mean()
            
            # Previous period metrics  
            prev_cost = prev_7_days['cost'].sum() if not prev_7_days.empty else 0
            prev_clicks = prev_7_days['clicks'].sum() if not prev_7_days.empty else 0
            prev_ctr = prev_7_days['ctr'].mean() if not prev_7_days.empty else 0
            
            # Anomaly Detection Rules
            reasons = []
            
            # Rule 1: Zombie Product (High cost, no clicks)
            if curr_cost > 30 and curr_clicks == 0:
                reasons.append(f"Zombie Product (Cost ${curr_cost:.2f}, 0 Clicks)")
            
            # Rule 2: CTR Decline > 30%
            if prev_ctr > 0 and curr_ctr < prev_ctr * 0.7:
                decline_pct = (prev_ctr - curr_ctr) / prev_ctr * 100
                reasons.append(f"CTR -{decline_pct:.0f}%")
            
            # Rule 3: Cost Efficiency Degradation (cost up, clicks down)
            if prev_cost > 0 and prev_clicks > 0:
                prev_cpc = prev_cost / prev_clicks
                curr_cpc = curr_cost / curr_clicks if curr_clicks > 0 else float('inf')
                if curr_cpc > prev_cpc * 1.5 and curr_cost > 20:
                    reasons.append(f"CPC +{((curr_cpc - prev_cpc) / prev_cpc * 100):.0f}%")
            
            if reasons:
                anomalies.append({
                    "id": str(item_id),
                    "item_id": str(item_id),
                    "title": str(title)[:50],  # Truncate long titles
                    "date": last_date.strftime('%Y-%m-%d'),
                    "curr_cost": float(curr_cost),
                    "prev_cost": float(prev_cost),
                    "curr_clicks": float(curr_clicks),
                    "prev_clicks": float(prev_clicks),
                    "curr_ctr": float(curr_ctr) if not pd.isna(curr_ctr) else 0.0,
                    "prev_ctr": float(prev_ctr) if not pd.isna(prev_ctr) else 0.0,
                    "reason": " & ".join(reasons)
                })
        
        # Sort by cost (highest cost issues first)
        anomalies.sort(key=lambda x: x['curr_cost'], reverse=True)
        return anomalies[:20]  # Limit to top 20

    except Exception as e:
        print(f"Product Anomaly Detection Error: {e}")
        return []
    finally:
        conn.close()


class AgentService:
    def __init__(self):
        print(f"Initializing Main Agent with model={MAIN_MODEL_NAME}")
        self.llm = main_llm
        
        self.tools = [scan_campaigns_for_anomalies, analyze_specific_table, call_pmax_agent, call_search_agent]
        self.llm_with_tools = self.llm.bind_tools(self.tools)
        
        self._init_prefs_db()

        workflow = StateGraph(AgentState)
        workflow.add_node("agent", self.call_model)
        workflow.add_node("tools", self.call_tools) # Use custom node
        workflow.set_entry_point("agent")
        workflow.add_conditional_edges("agent", self.should_continue, {"continue": "tools", "end": END})
        workflow.add_edge("tools", "agent")
        self.app = workflow.compile()

    def _init_prefs_db(self):
        conn = get_db_connection()
        cursor = conn.cursor()
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS user_preferences (
                table_name TEXT,
                item_identifier TEXT,
                is_pinned INTEGER DEFAULT 0,
                display_order INTEGER DEFAULT 0,
                PRIMARY KEY (table_name, item_identifier)
            )
        """)
        conn.commit()
        conn.close()

    def call_tools(self, state: AgentState):
        """
        Manual execution of tools to bypass ToolNode strictness.
        This ensures we can flexibly handle return types and injection.
        """
        messages = state['messages']
        last_message = messages[-1]
        
        if not hasattr(last_message, 'tool_calls') or not last_message.tool_calls:
            return {"messages": []}
        
        outputs = []
        
        for tool_call in last_message.tool_calls:
            tool_name = tool_call.get('name')
            tool_args = tool_call.get('args', {})
            tool_id = tool_call.get('id')
            
            # Robust Fallback for ID
            if not tool_id:
                print(f"âš ï¸ Warning: Missing tool_call_id for {tool_name}, generating random one.")
                tool_id = str(uuid.uuid4())
            
            print(f"ğŸ”§ Executing Tool: {tool_name} (ID: {tool_id})")
            
            result = "Error: Tool not found."
            
            try:
                if tool_name == 'scan_campaigns_for_anomalies':
                    result = scan_campaigns_for_anomalies(**tool_args)
                    
                elif tool_name == 'analyze_specific_table':
                    result = analyze_specific_table(**tool_args)
                
                elif tool_name == 'call_pmax_agent':
                    result = call_pmax_agent(**tool_args)

                elif tool_name == 'call_search_agent':
                    result = call_search_agent(**tool_args)

                else:
                    result = f"Error: Unknown tool '{tool_name}'."
                    
            except Exception as e:
                print(f"âŒ Tool Execution Error [{tool_name}]: {e}")
                result = f"Error executing tool {tool_name}: {str(e)}"
            
            # Ensure result is string
            if not isinstance(result, str):
                result = str(result)
            
            outputs.append(ToolMessage(content=result, tool_call_id=tool_id))
            
        return {"messages": outputs}

    def _sanitize_history(self, messages: List[BaseMessage]) -> List[BaseMessage]:
        """
        Converts previous Tool interactions into plain text to avoid strict 
        'thought_signature' checks by Gemini 3.0 on historical messages.
        Only the LAST message is kept as-is (if it's a User Request).
        """
        sanitized = []
        for i, msg in enumerate(messages):
            # Keep the System Prompt
            if isinstance(msg, SystemMessage):
                sanitized.append(msg)
                continue
                
            # Flatten Tool Interactions
            if isinstance(msg, AIMessage) and msg.tool_calls:
                # Convert 'AI calling tool' to text
                tool_names = [t['name'] for t in msg.tool_calls]
                sanitized.append(AIMessage(content=f"ğŸ¤” [Thinking History] I decided to call tools: {', '.join(tool_names)}."))
            
            elif isinstance(msg, ToolMessage):
                # Convert 'Tool Output' to text
                # Truncate very long outputs to save context
                content_preview = str(msg.content)[:500] + "..." if len(str(msg.content)) > 500 else str(msg.content)
                sanitized.append(HumanMessage(content=f"ğŸ”§ [Tool Output History]: {content_preview}"))
            
            else:
                # Keep normal text messages (User/AI Chat)
                sanitized.append(msg)
                
        return sanitized

    def call_model(self, state: AgentState):
        messages = state['messages']
        selected = state.get('selected_tables', [])
        
        # Build dynamic expertise list for the prompt
        available_experts = []
        for table_id in selected:
            if table_id in TABLE_EXPERT_KNOWLEDGE:
                info = TABLE_EXPERT_KNOWLEDGE[table_id]
                available_experts.append(f"- ã€{info['title']}ä¸“å®¶ã€‘: ä¸“æ³¨äº {info['focus']}ã€‚ä½¿ç”¨å·¥å…·æ—¶ä¼ å…¥ table_name='{table_id}'")

        expertise_section = "\n".join(available_experts) if available_experts else "å½“å‰æœªå¼€å¯ä»»ä½•ä¸“é¡¹æ·±åº¦è¯Šæ–­ (ç”¨æˆ·ä»…å…³æ³¨æ±‡æ€»æ•°æ®)ã€‚"

        # Ensure System Prompt
        if not isinstance(messages[0], SystemMessage):
            system_prompt = SystemMessage(content=f"""ä½ æ˜¯ AdsManager Main Agent (ä»»åŠ¡è°ƒåº¦å™¨)ã€‚
ä½ çš„èŒè´£æ˜¯å®æ—¶ç›‘æ§å¹¿å‘Šè¡¨ç°ï¼Œå¹¶åè°ƒâ€œä¸“é¡¹ä¸“å®¶â€è¿›è¡Œæ·±å…¥è¯Šæ–­ã€‚

**å½“å‰æ´»è·ƒçš„ä¸“é¡¹ä¸“å®¶ (ä»…é™ä»¥ä¸‹):**
{expertise_section}

**æ—¶é—´ç»´åº¦å†³ç­–:**
- ä½ å¿…é¡»æ ¹æ®ç”¨æˆ·çš„æé—®ï¼ˆå¦‚â€œåˆ†ææœ¬å‘¨â€ã€â€œåˆ†æ1æœˆ1æ—¥åˆ°18æ—¥â€ï¼‰æˆ–è€…é€šè¿‡ä¸Šä¸‹æ–‡æ„ŸçŸ¥æ¥å†³å®š `start_date` å’Œ `end_date`ã€‚
- å¦‚æœç”¨æˆ·æ²¡æœ‰æŒ‡å®šï¼Œé»˜è®¤ä½¿ç”¨æ•°æ®æˆªæ­¢æ—¥æœŸï¼ˆå¦‚ 2026-01-18ï¼‰çš„å‰7å¤©ã€‚
- å°†æ—¶é—´èŒƒå›´é€ä¼ ç»™ä¸‹å±‚å·¥å…·å‡½æ•°ã€‚

**å·¥ä½œæµç¨‹ä¸æ±‡æŠ¥åŸåˆ™:**
1. **æ˜¾æ€§åŒ–æ€è€ƒ**: åœ¨è°ƒç”¨ä»»ä½•å·¥å…·å‰ï¼Œä½ å¿…é¡»å…ˆè¾“å‡ºä¸€æ®µåˆ†ææ€è·¯ï¼ˆä¾‹å¦‚ï¼šâ€œç›‘æµ‹åˆ°ç»“æœ...æˆ‘å°†å¯åŠ¨...â€ï¼‰ã€‚
2. **å…¨é‡æ‰«æä¸æ±‡æŠ¥**: 
   - å½“è°ƒç”¨ `scan_campaigns_for_anomalies` æ—¶ï¼Œå¦‚æœè¿”å›ç»“æœåŒ…å«å¤šä¸ªå¹¿å‘Šç³»åˆ—ï¼Œä½ ã€å¿…é¡»ã€‘åœ¨æ±‡æ€»æ±‡æŠ¥ä¸­æ¶µç›–ã€æ‰€æœ‰ã€‘è¢«è¯†åˆ«å‡ºçš„å¼‚å¸¸ç³»åˆ—ã€‚ä¸¥ç¦åªä¿ç•™ä¸€ä¸ªæˆ–è¿‡åº¦ç®€åŒ–ã€‚
3. **å¤šç»´ä¸“å®¶è°ƒåº¦**: 
   - å¯¹äºæ¯ä¸€ä¸ªè¢«æ£€æµ‹å‡ºçš„å¼‚å¸¸ç³»åˆ—ï¼Œä½ åº”å½“æ ¹æ®å…¶â€œåˆæ­¥æ ¸å¿ƒåŸå› â€ï¼ˆRoot Causeï¼‰å†³å®šè°ƒé£å“ªäº›ä¸“å®¶ã€‚
   - å¦‚æœä¸€ä¸ªç³»åˆ—åŒæ—¶å­˜åœ¨ä¸»è¯æŸè€—å’Œäººç¾¤åå·®ï¼Œä½ åº”å½“åœ¨ä¸€ä¸ªè½®æ¬¡å†…åŒæ—¶å¯åŠ¨å¯¹åº”çš„å¤šä¸ªä¸“å®¶å·¥å…· `analyze_specific_table`ï¼ˆå¦‚ search_term + age + genderï¼‰ã€‚
4. **æ±‡æ€»æŠ¥å‘Š**: å°†æ‰€æœ‰ä¸“å®¶çš„æ·±åº¦åˆ†æç»“è®ºè¿›è¡Œèšåˆï¼Œç”Ÿæˆä¸“ä¸šã€ç»“æ„åŒ–ä¸”å…¨ä¸­æ–‡åŒ–çš„æœ€ç»ˆæ€»ç»“ã€‚

**åŸåˆ™:**
- åªæœ‰çœ‹åˆ°ç”¨æˆ·å‹¾é€‰äº†æŸä¸ªè¡¨å¯¹åº”çš„ Agentï¼Œä½ æ‰å…·å¤‡è°ƒé£è¯¥ä¸“å®¶çš„æƒé™ã€‚
- è¾“å‡ºå¿…é¡»ä¸“ä¸šã€å‡†ç¡®ã€‚æ·±åº¦åˆ†æç»“æœå¿…é¡»å‡†ç¡®æ ‡æ³¨æ•°æ®æ¥æºï¼ˆä¾‹å¦‚ "(æ•°æ®æ¥æº: channel è¡¨)"ï¼‰ã€‚
- **é€æ˜åŒ–æ‰§è¡Œ**: ç”¨æˆ·éœ€è¦çœ‹åˆ°ä½ å¯¹æ¯ä¸€ä¸ªå¼‚å¸¸ç³»åˆ—çš„ä¸“å®¶åˆ†æ´¾è¿‡ç¨‹ã€‚
""")
            messages = [system_prompt] + messages
            
        # SANITIZE HISTORY: Bypass 'thought_signature' check for past turns
        # We only strictly need structured objects for the *current* turn if we are processing it.
        # But here, we are invoking the model to *generate* the next step.
        # So previous steps can be flattened.
        safe_messages = self._sanitize_history(messages)

        print(f"ğŸ¤– Invoking Main Model ({MAIN_MODEL_NAME}) with {len(safe_messages)} safe messages...")
        response = self.llm_with_tools.invoke(safe_messages)
        return {"messages": [response]}

    def should_continue(self, state: AgentState):
        messages = state['messages']
        last_message = messages[-1]
        if last_message.tool_calls:
            return "continue"
        return "end"

    async def chat_stream(self, message: str, messages: list, selected_tables: list = None):
        input_messages = []
        if messages:
            for msg in messages:
                if msg.role == 'user':
                    input_messages.append(HumanMessage(content=msg.content))
                elif msg.role == 'agent':
                    input_messages.append(AIMessage(content=msg.content))
        
        input_messages.append(HumanMessage(content=message))
        
        # Initialize state with selected tables
        initial_state = {
            "messages": input_messages,
            "selected_tables": selected_tables or []
        }
        
        async for event in self.app.astream_events(initial_state, version="v1"):
            kind = event["event"]
            
            # Stream LLM text output
            if kind == "on_chat_model_stream":
                content = event["data"]["chunk"].content
                if content:
                    yield content
            
            # Stream tool calls (show which sub-agent/tool is being called)
            elif kind == "on_tool_start":
                tool_name = event.get("name", "Unknown Tool")
                tool_input = event.get("data", {}).get("input", {})
                
                # Send a special marker for tool calls
                if tool_name == "scan_campaigns_for_anomalies":
                    yield f"\n\nğŸ” **[è°ƒç”¨å·¥å…·]** æ‰«ææ‰€æœ‰å¹¿å‘Šç³»åˆ—...\n\n"
                elif tool_name == "call_pmax_agent":
                    campaign = tool_input.get("campaign_name", "Unknown")
                    yield f"\n\nğŸ¯ **[è°ƒç”¨ PMax Agent]** åˆ†æ {campaign}...\n\n"
                elif tool_name == "analyze_specific_table":
                    campaign = tool_input.get("campaign_name", "Unknown")
                    table = tool_input.get("table_name", "Unknown")
                    yield f"\n\nğŸ©º **[ä¸“é¡¹åˆ†æ]** æ­£åœ¨è°ƒé£ä¸“å®¶åˆ†æ {campaign} çš„ {table} æ•°æ®...\n\n"
                elif tool_name == "call_search_agent":
                    campaign = tool_input.get("campaign_name", "Unknown")
                    yield f"\n\nğŸ” **[è°ƒç”¨ Search Agent]** åˆ†æ {campaign}...\n\n"
            
            # Stream tool results (optional, can show completion)
            elif kind == "on_tool_end":
                tool_name = event.get("name", "Unknown Tool")
                # You can optionally show tool completion
                # yield f"\nâœ… [{tool_name}] å®Œæˆ\n"

    def get_tables(self):
        conn = get_db_connection()
        cursor = conn.cursor()
        cursor.execute("SELECT name FROM sqlite_master WHERE type='table';")
        tables = [row[0] for row in cursor.fetchall()]
        conn.close()
        return tables

    def get_table_data(self, table_name, start_date: str = None, end_date: str = None):
        conn = get_db_connection()
        conn.row_factory = sqlite3.Row
        cursor = conn.cursor()
        try:
            pk_col = 'campaign' 
            if table_name == 'search_term': pk_col = 'search_term'
            elif table_name == 'product': pk_col = 'item_id' 
            elif table_name == 'asset': pk_col = 'ad_group' 
            elif table_name == 'audience': pk_col = 'audience_segment'
            elif table_name == 'channel': pk_col = 'channels'
            
            where_clause = ""
            params = []
            
            if start_date and end_date:
                where_clause = "WHERE t.date >= ? AND t.date <= ?"
                params = [start_date, end_date]
            elif start_date:
                where_clause = "WHERE t.date >= ?"
                params = [start_date]
            elif end_date:
                where_clause = "WHERE t.date <= ?"
                params = [end_date]
            
            query = f"""
                SELECT t.*, 
                       COALESCE(p.is_pinned, 0) as _pinned, 
                       COALESCE(p.display_order, 999999) as _order
                FROM {table_name} t
                LEFT JOIN user_preferences p 
                ON p.table_name = '{table_name}' AND p.item_identifier = t.{pk_col}
                {where_clause}
                ORDER BY _pinned DESC, _order ASC, date DESC
            """
            
            cursor.execute(query, params)
            rows = cursor.fetchall()
            
            if not rows: return {"columns": [], "data": []}

            columns = [description[0] for description in cursor.description]
            display_columns = [c for c in columns if c not in ['_pinned', '_order']]
            
            data = [dict(row) for row in rows]
            return {"columns": display_columns, "data": data}
        except Exception as e:
            return {"error": str(e)}
        finally:
            conn.close()

    def get_campaign_anomalies(self, target_date: str = None):
        """Wrapper for standalone logic"""
        return get_campaign_anomalies_logic(target_date)

    def get_product_anomalies(self, target_date: str = None):
        """Wrapper for product anomaly detection"""
        return get_product_anomalies_logic(target_date)

    def update_preference(self, table_name: str, item_identifier: str, is_pinned: int = None, display_order: int = None):
        conn = get_db_connection()
        cursor = conn.cursor()
        try:
            cursor.execute("SELECT * FROM user_preferences WHERE table_name=? AND item_identifier=?", (table_name, item_identifier))
            exists = cursor.fetchone()
            
            if exists:
                if is_pinned is not None:
                    cursor.execute("UPDATE user_preferences SET is_pinned=? WHERE table_name=? AND item_identifier=?", (is_pinned, table_name, item_identifier))
                if display_order is not None:
                    cursor.execute("UPDATE user_preferences SET display_order=? WHERE table_name=? AND item_identifier=?", (display_order, table_name, item_identifier))
            else:
                pinned = is_pinned if is_pinned is not None else 0
                order = display_order if display_order is not None else 0
                cursor.execute("INSERT INTO user_preferences (table_name, item_identifier, is_pinned, display_order) VALUES (?, ?, ?, ?)", (table_name, item_identifier, pinned, order))
            
            conn.commit()
            return {"status": "success"}
        except Exception as e:
            return {"error": str(e)}
        finally:
            conn.close()

    def reset_preferences(self, table_name: str):
        conn = get_db_connection()
        cursor = conn.cursor()
        try:
            cursor.execute("DELETE FROM user_preferences WHERE table_name=?", (table_name,))
            conn.commit()
            return {"status": "success"}
        except Exception as e:
            return {"error": str(e)}
        finally:
            conn.close()

    def get_campaign_details(self, campaign_name: str, start_date: str = None, end_date: str = None):
        """Get all related data for a specific campaign from all tables"""
        tables = [
            'search_term', 'channel', 'asset', 
            'audience', 'age', 'gender', 
            'location_by_cities_all_campaign', 'ad_schedule'
        ]
        
        result = {}
        conn = get_db_connection()
        conn.row_factory = sqlite3.Row
        cursor = conn.cursor()
        
        try:
            for table in tables:
                try:
                    where_conditions = []
                    params = []

                    # 1. Campaign Filter (Skip for product)
                    if table != 'product':
                        campaign_col = 'campaign'
                        if table == 'channel':
                            campaign_col = 'campaigns'
                        where_conditions.append(f"{campaign_col} = ?")
                        params.append(campaign_name)
                    
                    # 2. Date Filter
                    if start_date:
                        where_conditions.append("date >= ?")
                        params.append(start_date)
                    if end_date:
                        where_conditions.append("date <= ?")
                        params.append(end_date)
                    
                    where_clause = " WHERE " + " AND ".join(where_conditions) if where_conditions else ""
                    
                    query = f"SELECT * FROM {table}{where_clause}"
                    
                    # Check for sort column
                    check_query = f"PRAGMA table_info({table})"
                    cursor.execute(check_query)
                    cols = [info[1] for info in cursor.fetchall()]
                    
                    if 'cost' in cols:
                        query += " ORDER BY date DESC, CAST(cost AS REAL) DESC"
                    else:
                        query += " ORDER BY date DESC"
                    
                    cursor.execute(query, tuple(params))
                    
                    rows = cursor.fetchall()
                    
                    if rows:
                        columns = [description[0] for description in cursor.description]
                        data = [dict(row) for row in rows]
                        result[table] = {"columns": columns, "data": data}
                    else:
                        result[table] = {"columns": [], "data": []}
                except Exception as e:
                    result[table] = {"error": str(e), "columns": [], "data": []}
            
            return result
        finally:
            conn.close()

    def _init_custom_rules_db(self):
        """Initialize the custom rules table if it doesn't exist"""
        conn = get_db_connection()
        cursor = conn.cursor()
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS agent_custom_rules (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                table_name TEXT NOT NULL,
                rule_prompt TEXT,
                is_active INTEGER DEFAULT 1,
                created_at TEXT DEFAULT CURRENT_TIMESTAMP,
                updated_at TEXT DEFAULT CURRENT_TIMESTAMP
            )
        """)
        conn.commit()
        conn.close()

    def save_custom_rule(self, table_name: str, rule_prompt: str):
        """Save or update a custom rule"""
        self._init_custom_rules_db()
        conn = get_db_connection()
        cursor = conn.cursor()
        try:
            # Check if rule already exists for this table
            cursor.execute("SELECT id FROM agent_custom_rules WHERE table_name = ?", (table_name,))
            existing = cursor.fetchone()
            
            if existing:
                # Update existing rule
                cursor.execute("""
                    UPDATE agent_custom_rules 
                    SET rule_prompt = ?, updated_at = CURRENT_TIMESTAMP 
                    WHERE table_name = ?
                """, (rule_prompt, table_name))
            else:
                # Insert new rule
                cursor.execute("""
                    INSERT INTO agent_custom_rules (table_name, rule_prompt) 
                    VALUES (?, ?)
                """, (table_name, rule_prompt))
            
            conn.commit()
            return {"status": "success", "message": "Custom rule saved"}
        except Exception as e:
            return {"status": "error", "message": str(e)}
        finally:
            conn.close()

    def get_custom_rules(self, table_name: str):
        """Get custom rules for a specific table"""
        self._init_custom_rules_db()
        conn = get_db_connection()
        cursor = conn.cursor()
        try:
            cursor.execute("""
                SELECT rule_prompt, created_at, updated_at 
                FROM agent_custom_rules 
                WHERE table_name = ? AND is_active = 1
            """, (table_name,))
            result = cursor.fetchone()
            
            if result:
                return {
                    "rule_prompt": result[0],
                    "created_at": result[1],
                    "updated_at": result[2]
                }
            else:
                return {"rule_prompt": None}
        except Exception as e:
            return {"error": str(e)}
        finally:
            conn.close()

    def get_agent_default_prompt(self, table_name: str):
        """Get the default prompt/rules for a specific agent from TABLE_EXPERT_KNOWLEDGE"""
        if table_name in TABLE_EXPERT_KNOWLEDGE:
            knowledge = TABLE_EXPERT_KNOWLEDGE[table_name]
            return {
                "title": knowledge.get("title", ""),
                "focus": knowledge.get("focus", ""),
                "metrics_desc": knowledge.get("metrics_desc", ""),
                "expert_rules": knowledge.get("expert_rules", "").strip(),
                "default_prompt": f"""ã€{knowledge.get("title", "")}ã€‘
ä¸“æ³¨é¢†åŸŸ: {knowledge.get("focus", "")}
æŒ‡æ ‡è¯´æ˜: {knowledge.get("metrics_desc", "")}

ä¸“å®¶è§„åˆ™:
{knowledge.get("expert_rules", "").strip()}"""
            }
        else:
            return {"error": f"Unknown agent: {table_name}", "default_prompt": ""}

