"""
SEO æ•°æ®åŒæ­¥è„šæœ¬
ä» Google Search Console API æ‹‰å–æ•°æ®å¹¶ä¿å­˜åˆ°æœ¬åœ°æ•°æ®åº“ï¼ˆæŒ‰æ—¥æœŸï¼‰

è¿è¡Œæ–¹å¼ï¼š
python sync_seo_data.py
"""

import os
import sys
import sqlite3
from datetime import datetime, timedelta
from concurrent.futures import ThreadPoolExecutor, as_completed

sys.path.append(os.path.dirname(os.path.abspath(__file__)))

def get_db_connection():
    db_path = os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), 'ads_data.sqlite')
    return sqlite3.connect(db_path)

def fetch_meta(url):
    """çˆ¬å–å•ä¸ªURLçš„Metaä¿¡æ¯"""
    import requests
    from bs4 import BeautifulSoup
    
    meta_title = ""
    meta_description = ""
    
    try:
        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'}
        resp = requests.get(url, headers=headers, timeout=5)
        soup = BeautifulSoup(resp.content, 'html.parser')
        
        if soup.title and soup.title.string:
            meta_title = soup.title.string.strip()
        
        meta_desc = soup.find('meta', attrs={'name': 'description'})
        if meta_desc and meta_desc.get('content'):
            meta_description = meta_desc['content'].strip()
    except:
        pass
    
    return url, meta_title, meta_description

def sync_seo_data():
    """ä» Google Search Console æ‹‰å–æ•°æ®å¹¶ä¿å­˜åˆ°æ•°æ®åº“ï¼ˆæŒ‰æ—¥æœŸèŒƒå›´æ±‡æ€»ï¼‰"""
    from google.oauth2 import service_account
    from googleapiclient.discovery import build
    
    KEY_FILE_PATH = os.path.join(os.path.dirname(__file__), 'zhiyuanzhongyi-b17bb896700a.json')
    SITE_URL = 'sc-domain:baofengradio.co.uk'
    
    # æ—¥æœŸèŒƒå›´ï¼šä»Šå¤©åˆ°å‰3ä¸ªæœˆ
    end_date = datetime.now() - timedelta(days=3)
    start_date = end_date - timedelta(days=90)
    
    print(f"ğŸ“… æ—¥æœŸèŒƒå›´: {start_date.strftime('%Y-%m-%d')} åˆ° {end_date.strftime('%Y-%m-%d')}")
    
    try:
        print("ğŸ”— è¿æ¥ Google Search Console API...")
        creds = service_account.Credentials.from_service_account_file(
            KEY_FILE_PATH, scopes=['https://www.googleapis.com/auth/webmasters.readonly']
        )
        service = build('searchconsole', 'v1', credentials=creds)
        
        # å‡†å¤‡æ•°æ®åº“
        conn = get_db_connection()
        cursor = conn.cursor()
        
        # åˆ›å»ºæ–°è¡¨ï¼ˆå¸¦æ—¥æœŸå­—æ®µï¼‰
        cursor.execute('DROP TABLE IF EXISTS seo_pages')
        cursor.execute('''
            CREATE TABLE seo_pages (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                url TEXT,
                clicks INTEGER DEFAULT 0,
                impressions INTEGER DEFAULT 0,
                ctr REAL DEFAULT 0,
                position REAL DEFAULT 0,
                meta_title TEXT,
                meta_description TEXT,
                start_date TEXT,
                end_date TEXT,
                updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        ''')
        conn.commit()
        
        # è·å–æ•´ä½“æ•°æ®
        request = {
            'startDate': start_date.strftime('%Y-%m-%d'),
            'endDate': end_date.strftime('%Y-%m-%d'),
            'dimensions': ['page'],
            'rowLimit': 1000
        }
        
        print("ğŸ“Š æ­£åœ¨è·å–æœç´¢æ•°æ®...")
        response = service.searchanalytics().query(siteUrl=SITE_URL, body=request).execute()
        rows = response.get('rows', [])
        print(f"âœ… è·å–åˆ° {len(rows)} æ¡æ•°æ®")
        
        # å‡†å¤‡æ•°æ®
        pages_data = []
        for row in rows:
            pages_data.append({
                'url': row['keys'][0],
                'clicks': row.get('clicks', 0),
                'impressions': row.get('impressions', 0),
                'ctr': round(row.get('ctr', 0) * 100, 2),
                'position': round(row.get('position', 0), 1)
            })
        
        # å¹¶è¡Œçˆ¬å– Meta ä¿¡æ¯
        print("ğŸš€ å¹¶è¡Œçˆ¬å– Meta ä¿¡æ¯ (10 çº¿ç¨‹)...")
        urls = [p['url'] for p in pages_data]
        meta_map = {}
        
        with ThreadPoolExecutor(max_workers=10) as executor:
            futures = {executor.submit(fetch_meta, url): url for url in urls}
            done = 0
            for future in as_completed(futures):
                url, title, desc = future.result()
                meta_map[url] = {'title': title, 'description': desc}
                done += 1
                if done % 50 == 0:
                    print(f"   å·²å¤„ç† {done}/{len(urls)}...")
        
        print(f"âœ… Meta ä¿¡æ¯çˆ¬å–å®Œæˆ")
        
        # ä¿å­˜æ•°æ®ï¼ˆè®°å½•æ—¥æœŸèŒƒå›´ï¼‰
        start_str = start_date.strftime('%Y-%m-%d')
        end_str = end_date.strftime('%Y-%m-%d')
        
        for page in pages_data:
            meta = meta_map.get(page['url'], {})
            cursor.execute('''
                INSERT INTO seo_pages (url, clicks, impressions, ctr, position, meta_title, meta_description, start_date, end_date, updated_at)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, CURRENT_TIMESTAMP)
            ''', (page['url'], page['clicks'], page['impressions'], page['ctr'], page['position'], 
                  meta.get('title', ''), meta.get('description', ''), start_str, end_str))
        
        conn.commit()
        conn.close()
        
        print(f"\nğŸ‰ åŒæ­¥å®Œæˆï¼å…±ä¿å­˜ {len(pages_data)} æ¡æ•°æ®")
        print(f"   æ—¥æœŸèŒƒå›´: {start_str} è‡³ {end_str}")
        
    except Exception as e:
        print(f"âŒ åŒæ­¥å¤±è´¥: {e}")
        raise

if __name__ == "__main__":
    sync_seo_data()
